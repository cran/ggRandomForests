\documentclass[nojss]{jss}

%\usepackage{setspace}
% \usepackage[sc]{mathpazo}
\usepackage{amsmath}
% %\usepackage{geometry} 
% %\geometry{verbose, tmargin = 1.25cm, bmargin = 1.25cm, lmargin = 1.25cm, rmargin = 1.25cm}
% \setcounter{secnumdepth}{2}
% \setcounter{tocdepth}{2}

\usepackage{longtable}
\usepackage{colortbl, xcolor}
\usepackage{booktabs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{ggRandomForests}
%\VignetteIndexEntry{Random Forests for Survival ggRandomForests packages}                     
%\VignetteKeywords{random forest, survival, VIMP, minimal depth}                                  
%\VignetteDepends{ggRandomForests}                   
%\VignettePackage{ggRandomForests} 

%% almost as usual
\author{John Ehrlinger 
\and Eugene H. Blackstone\\Cleveland Clinic}

\title{\pkg{ggRandomForests}: Random Forests for Survival}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Ehrlinger et.\ al.} %% comma-separated
\Plaintitle{ggRandomForests: Random Forests for Survival} %% without formatting
\Shorttitle{Random Forests for Survival}

%% an abstract and keywords
\Abstract{ 
Random Forests~\citep{Breiman:2001} (RF) are a fully non-parametric statistical method requiring no distributional assumptions on covariate relation to the response. RF are a robust, nonlinear technique that optimizes predictive accuracy by fitting an ensemble of trees to stabilize model estimates. Random Forests for survival~\citep{Ishwaran:2007a, Ishwaran:2008} (RF-S) are an extension of Breiman's RF techniques to survival settings, allowing efficient non-parametric analysis of time to event data. The \pkg{randomForestSRC} package~\citep{Ishwaran:RFSRC:2014} is a unified treatment of Breiman's random forests for survival, regression and classification problems.

Predictive accuracy make RF an attractive alternative to parametric models, though complexity and interpretability of the forest hinder wider application of the method. We introduce the \pkg{ggRandomForests} package, tools for creating and plotting data structures to visually understand random forest models grown in \proglang{R} with the \pkg{randomForestSRC} package. The \pkg{ggRandomForests} package is structured to extract intermediate data objects from \pkg{randomForestSRC} objects and generate figures using the \pkg{ggplot2}~\citep{Wickham2009} graphics package.

This document is formatted as a tutorial for using the \pkg{randomForestSRC} for building random forests for survival and \pkg{ggRandomForests} package for investigating how the forest is constructed. This tutorial uses the Primary Biliary Cirrhosis (PBC) Data from the Mayo Clinic~\citep{fleming:1991} available in the \pkg{randomForestSRC} package. We use Variable Importance measure (VIMP)~\citep{Breiman:2001} as well as Minimal Depth~\citep{Ishwaran:2010}, a property derived from the construction of each tree within the forest, to assess the impact of variables on forest prediction. We will also demonstrate the use of variable dependence plots~\citep{Friedman00greedyfunction} to aid interpretation RF results in different response settings. We also will investigate interactions between covariates to demonstrate the strength of the Random Forest method in survival settings.
}
\Keywords{random forest, survival, VIMP, minimal depth, \proglang{R}, \pkg{randomForestSRC}}
\Plainkeywords{random forest, survival, VIMP, minimal depth, R, randomForestSRC}
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
John Ehrlinger\\
Quantitative Health Sciences\\
Lerner Research Institute\\
Cleveland Clinic\\
9500 Euclid Ave\\
Cleveland, Ohio 44195\\
% Telephone: + 41/0/44634-4643 \\
% Fax: + 41/0/44634-4386 \\
E-mail: \email{john.ehrlinger@gmail.com}\\
URL: \url{http://www.lerner.ccf.org/qhs/people/ehrlinj/}
}

%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: + 43/1/31336-5053
%% Fax: + 43/1/31336-734

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<setup, include = FALSE, cache = FALSE, echo = FALSE>>= 
library(knitr)
knitr::render_sweave() 
# set global chunk options for knitr. These can be changed in the header for each individual R code chunk
opts_chunk$set(fig.path = 'figure/rfs-', 
        fig.align = 'center', 
        fig.pos = "!htpb", 
        fig.show = 'hold', 
        fig.height = 3, 
        fig.width = 4, 
        size = 'footnotesize', 
        prompt = TRUE, 
        highlight = FALSE, 
        comment = NA, 
        echo = FALSE, results = FALSE, message = FALSE, warning = FALSE, 
        error = FALSE, dev = 'pdf', prompt = TRUE)

# Setup the R environment
options(object.size = Inf, expressions = 100000, memory = Inf, 
    replace.assign = TRUE, width = 90, prompt = "R> ")

#################
# Load_packages #
#################
library(ggplot2) # Graphics engine for generating all types of plots

library(dplyr) # Better data manipulations
library(tidyr)

library(ggRandomForests)

# Analysis packages.
library(randomForestSRC) 
library(RColorBrewer)

library(xtable)

options(mc.cores = 1, rf.cores = 1)

#########################################################################
# Default computation settings
#########################################################################
theme_set(theme_bw())
event.marks <- c(1, 4)
event.labels <- c(FALSE, TRUE)
strCol <- brewer.pal(3, "Set1")
strCol <- strCol[c(2, 1, 3)]
alpha <- .3
## Set the event/censor marks. Want open circle for censored, and 
## x for events 
event.marks = c(1, 4)
event.labels = c(FALSE, TRUE)
strCol <- brewer.pal(3, "Set1")
strCol <- c(strCol[2], strCol[1])

@
\begin{document}
% -----------------------------------------------------
\section{About this document}
% -----------------------------------------------------
This document is an introduction to the \pkg{ggRandomForests} \proglang{R} package. The aim of this introduction is to provide a detailed user guide to \pkg{ggRandomForests} as well as provide a tutorial to building a Random Forest Survival model with the \pkg{randomForestSRC} package. Our attempt is to build simple, reproducible worked examples with the Primary Biliary Cirrhosis (PBC) Data from the Mayo Clinic.

This document is available as a vignette within \pkg{ggRandomForests} package, available from the Comprehensive \proglang{R} Archive Network via \url{http://CRAN.R-project.org/package = ggRandomForests}.

% -----------------------------------------------------
\section{Introduction} \label{S:introduction}
% -----------------------------------------------------

Random Forests~\citep{Breiman:2001} (RF) are a robust, non-parametric statistical method that optimizes predictive accuracy by averaging an ensemble of tree models. Random Forests are not parsimonious, utilizing all provided variables in predicting the specified outcome. It does not require prior knowledge of the parametric relation of variables (linearity or non-linearity) to the response, or of interactions between variables. RF chooses the most important variables by assessing variable impact on the predictive ability of the forest of trees.

A Random Forest is built up by bagging~\citep{Breiman:1996} a collection of classification and regression trees~\citep{cart:1984} (CART). The method uses a set of $B$ bootstrap~\citep{bootstrap:1994} samples, growing a set of independent tree models on each sub-sample of the population. Trees are grown by recursively partitioning the population based on optimization of a split rule over the $p$ dimensional covariate space. At each split, a subset of $m \le p$ candidate variables are chosen for the splitting. Each node is split into two daughter nodes by maximizing the separation of observations according the split rule. In regression trees, node impurity is measured by mean squared error, whereas in classification problems, the Gini index is used~\citep{FriedmanGreedyfunction:2000}. Each subsequent daughter node is then split until the process reaches the stopping criteria of either node purity or node member size defining the set of terminal (unsplit) nodes for the tree. Random Forests sort each observation into one unique terminal node per tree. The Random Forest estimate for each observation is calculated by aggregation, averaging (regression) or votes (classification), the terminal node results across the collection of $B$ trees. 

One advantage of Random Forests is a built in generalization error estimate. Each bootstrap sample selects approximately $63.2\%$ of the population on average. The remaining $36.8\%$ of observations, the Out-of-Bag~\citep{BreimanOOB:1996e} (OOB) sample, can be used as a hold out test set for each tree. An OOB prediction error estimate can be calculated for each observation by predicting the response over the set of trees which were NOT trained with that particular observation. Out-of-Bag prediction error estimates have been shown to be nearly identical to $n$--fold cross validation estimates~\citep{StatisticalLearning:2009}. This feature of Random Forests allows us to obtain both model fit and validation in one pass of the algorithm.

\subsection{Random Forests for Survival}\label{S:rfs}
Random Forests for survival~\citep{Ishwaran:2007, Ishwaran:2008} (RF-S) are an extension of~\cite{Breiman:2001} Random Forests for right censored time to event data. A forest of survival trees is grown using a log-rank splitting rule to select the optimal candidate variables. Survival estimate for each observation are constructed with a Kaplan--Meier (KM) estimator within each terminal node, at each event time. 

Random Forests for survival adaptively discover nonlinear effects and interactions and are fully nonparametric. Averaging over trees, with randomizing while growing a tree, enables RF-S to approximate complex survival functions, including non-proportional hazards, while maintaining low prediction error. \cite{Ishwaran:2010a} showed that RF-S is uniformly consistent and that survival forests have a uniform approximating property in finite-sample settings, a property not possessed by individual survival trees.

\subsection{ggRandomForests}
The \pkg{randomForestSRC} package is a mature analysis and research random forest implementation under rapid development. The package includes diagnostic and post processing functions for analysis and visualizations of randomForest model properties. However, in our research we frequently found it difficult to manipulate the standard figures directly produced with the \pkg{randomForestSRC} package. 

In order to simplify these manipulations, we developed the \pkg{ggRandomForests} package. We attempted to follow two design principles in this development:
\begin{itemize}
\item Model/View separation: The package originally designed to generating \pkg{ggplot2}~\cite{Wickham2009} figures for random forest objects. However, some users would prefer to use other graphing methods within \proglang{R} or outside of it. To help users, we separate the data generation and the figure generation into two separate operations. 

\item Modular: We strive to create a modular design by following the \emph{do one thing well} philosphy. Each function operates on one \pkg{randomForestSRC} object to create only one data object or figure type.
\end{itemize}

To demonstrate using the \pkg{ggRandomForests} package, we organize this document as follows. In Section~\ref{S:rfsrcGrow} we outline growing a random forest for each of the classification, regression and survival settings with the \pkg{randomForestSRC} package. We use the \pkg{ggRandomForests} package to begin exploring random forest convergence and prediction. In Section~\ref{S:variableIdentification} we discuss how variables contribute to the random forest prediction using the Variable Importance (VIMP) and Minimal Depth measures.

Once we have an idea which variables are most informative in minimizing forest prediction error, we turn our focus to how the variables are related to the forest prediction. Because Random Forests are non-linear and non-parametric predictors, we can use variable dependence (Section~\ref{S:variableDependence}) to examine where each observation contributes to model prediction as a function of specific covariate values. Partial dependence (Section~\ref{S:partialDependence}) gives us a risk adjust view of the predictor dependence on a variable. We then find two way interactions using minimal depth in Section~\ref{S:interactions} and use conditional plots in Section~\ref{S:coplots} to look variable interactions in an intuitive manner.

\section{Data Summary: Primary Biliary Cirrhosis (PBC) Data}

Data from the Mayo Clinic trial in primary biliary cirrhosis (PBC) of the liver conducted between 1974 and 1984. A total of 424 PBC patients, referred to Mayo Clinic during that ten-year interval, met eligibility criteria for the randomized placebo controlled trial of the drug D-penicillamine. The first 312 cases in the data set participated in the randomized trial and contain largely complete data.

<<datastep>>= 
data(pbc, package = "randomForestSRC")

## Set modes correctly. For binary variables: transform to logical
## Check for range of 0, 1
## There is probably a better way to do this.
for(ind in 1:dim(pbc)[2]){
 if(!is.factor(pbc[, ind])){
  if(length(unique(pbc[which(!is.na(pbc[, ind])), ind]))<= 2) {
   if(sum(range(pbc[, ind], na.rm = TRUE) ==  c(0, 1)) ==  2){
    pbc[, ind] <- as.logical(pbc[, ind])
   }
  }
 }else{
  if(length(unique(pbc[which(!is.na(pbc[, ind])), ind]))<= 2) {
   if(sum(sort(unique(pbc[, ind])) ==  c(0, 1)) ==  2){
    pbc[, ind] <- as.logical(pbc[, ind])
   }
   if(sum(sort(unique(pbc[, ind])) ==  c(FALSE, TRUE)) ==  2){
    pbc[, ind] <- as.logical(pbc[, ind])
   }
  }
 }
 if(!is.logical(pbc[, ind]) & 
    length(unique(pbc[which(!is.na(pbc[, ind])), ind]))<= 5) {
  pbc[, ind] <- factor(pbc[, ind])
 }
}
# Convert age to years
pbc$age <- pbc$age/364.24
pbc$years <- pbc$days/364.24
pbc <- pbc %>% select(-days)

cls <- sapply(pbc, class) 

labels <- c("censoring indicator", "1 = D-penicillamine, 2 = placebo", 
"age in years", "0 = female, 1 = male", "presence of asictes", "presence of hepatomegaly", 
"presence of spiders", "presence of edema", "serum bilirubin in mg/dl", 
"serum cholesterol in mg/dl", "albumin in gm/dl", "urine copper in ug/day", 
"alkaline phosphatase in U/liter", "SGOT in U/ml", "triglicerides in mg/dl", 
"platelets per cubic ml/1000", "prothrombin time in seconds", "histologic stage of disease", 
"survival time in years")

dta.labs <- data.frame(cbind(names = colnames(pbc), label = labels, type = cls))
@

<<dta-table, results = "asis">>= 
rws <- seq(1, (nrow(dta.labs)), by = 2)
col <- rep("\\rowcolor[gray]{0.95}", length(rws))

print(xtable(dta.labs %>% select(-names), 
       caption = "PBC Data field descriptions", 
       label = "T:dataLabs", 
       digits = 3), 
   size = 'footnotesize', # fontsize
   booktabs = TRUE, 
   add.to.row = list(pos = as.list(rws), command = col), 
   command =  c('\\toprule' , 
          '\\midrule' , 
          '\\bottomrule ')
)
@

%' Figure~\ref{fig:dtaViewCat} and~\ref{fig:dtaView} are included to verify the data import step. 
%' 
%' <<dtaViewCat, fig.cap = "Categorical Variable measures over time.", fig.width = 7.5, fig.height = 4>>= 
%' dta.tmp<-pbc[, c(which(cls ==  "factor"), 
%'         which(cls ==  "logical"), 
%'          which(colnames(pbc) %in% c("age")))]
%' plt.dta <- dta.tmp %>% gather(variable, value, -age)
%' 
%' dtaView<- ggplot(plt.dta, aes(x = age, fill = value)) + 
%'  geom_histogram(binwidth = 5, color = "black") + 
%'  labs(x = "Age", y = "") + 
%'  facet_wrap(~variable, ncol = 3) + 
%'  theme(legend.position = "none") + 
%'  scale_fill_brewer(palette = "Paired", na.value = "lightgrey")
%' 
%' suppressWarnings(show(dtaView))
%' 
%' @
%' 
%' <<dtaView, fig.cap = "Continuous Variable measures over time.", fig.width = 7.5, fig.height = 4>>= 
%' dta.tmp <- pbc[, c(which(cls ==  "numeric"), which(cls ==  "integer"), 
%'           which(colnames(pbc) ==  "status"))]
%' 
%' plt.dta <- dta.tmp %>% gather(variable, value, -age, -status)
%' 
%' dtaView<- ggplot(plt.dta, aes(x = age, y = value, color = status, shape = status)) + 
%'  geom_point(alpha = alpha) + 
%'  geom_rug(aes(x = age), data = plt.dta %>% filter(is.na(value))) + 
%'  labs(x = "Age", y = "") + 
%'  facet_wrap(~variable, scales = "free_y", ncol = 3) + 
%'  scale_color_manual(values = strCol, na.value = "black", drop = FALSE, 
%'           labels = event.labels) + 
%'  scale_shape_manual(values = event.marks, labels = event.labels) + 
%'  theme(legend.position = "none")
%' 
%' suppressWarnings(show(dtaView))
%' @

% \doublespacing
\section{Growing the Random Forest}


<<rfsrc, echo = TRUE, eval = FALSE>>= 
pbc_rf <- rfsrc(Surv(years, status) ~ ., data = pbc, 
         ntree = 2000, 
         na.action = "na.impute", 
         fast.restore = TRUE)
@


<<read-forest, echo = FALSE, results = FALSE>>= 
data(pbc_rf, package="ggRandomForests")
pbc_rf
@

<<errorPlot, fig.cap = "RSF prediction error estimates">>= 
ggerr <- gg_error(pbc_rf)
plot(ggerr)+
  coord_cartesian(ylim=c(.09,.31))
@

Figure~\ref{fig:rfsrc-plot} shows the predicted survival from an RF-S model, where censored device prediction is colored in blue, and devices experiencing a thrombosis event are colored in red. 
<<rfsrc-plot, fig.cap = "PBC Survival">>= 
ggRFsrc <- plot.gg_rfsrc(pbc_rf, alpha = .2) + 
 scale_color_manual(values = strCol) + 
 theme(legend.position = "none") + 
 labs(y = "Survival Probability", x = "time (years)")
ggRFsrc
@

\subsection{Forest Imputation for missing values}\label{S:imputation}

The randomForests package~\citep{liaw:2002} include a forest imputation method within the randomForest package. 

We impute missing data (both x and y-variables) using a modification of the missing data algorithm of~\cite{Ishwaran:2008}. Prior to splitting a node, missing data for a variable is imputed by randomly drawing values from non-missing in-bag data. The purpose of the imputed data is to make it possible to assign cases to daughter nodes in the event the node is split on a variable with missing data. Imputed data is however not used to calculate the split-statistic which uses non-missing data only. Following a node split, imputed data are reset to missing and the process is repeated until terminal nodes are reached. Missing data is then imputed using OOB non-missing terminal node data. For integer valued variables and censoring indicators, imputation uses a maximal class rule, whereas continuous variables and survival time use a mean rule.

The proximity matrix from the randomForest is used to update the imputation of the NAs. For continuous predictors, the imputed value is the weighted average of the non-missing obervations, where the weights are the proximities. For categorical predictors, the imputed value is the category with the largest average proximity. This process is iterated iter times.

Regardless of what method is used, records in which all outcome and x-variable information are missing are removed from the forest analysis. Variables having all missing values are also removed.


\section{Variable Selection}
Unlike in the linear model settings, Random Forests does not require explicitly specify the functional form of the covariates to the response. Instead, we ascertain which variables contribute to the Random Forest estimates by querying the forest for variable usage. 

\subsection{Variable Importance}\label{S:vimp}
Unlike in the linear model settings, Random Forests does not require explicitly specify the functional form of the covariates to the response. Instead, we ascertain which variables contribute to the Random Forest estimates by querying the forest for variable usage. 

Variable importance (VIMP) was originally defined in CART using a measure involving surrogate variables (see Chapter 5 of~\cite{cart:1984}). The most popular VIMP method to date, adopts a prediction error approach involving "noising-up" a variable. VIMP for a variable $x_v$ is the difference between prediction error when $x_v$ is noised up by permuting its value randomly, compared to prediction error under the original predictor~\citep{Breiman:2001, liaw:2002, Ishwaran:2007, Ishwaran:2008}.

Since VIMP is the absolute difference between prediction errors before and after permutation, a large VIMP value indicates that misspecification of that variable detracts from the predictive accuracy of the forest. VIMP close to zero indicates the variable contributes nothing to predictive accuracy, and negative values indicate the predictive accuracy improves when the variable is mispecified. In the later case, we assume noise is more informative than the variable. As such, we ignore variables with negative and near zero values of VIMP, relying on large positive values to indicate that the predictive power of the forest is dependent on those variables. 

In Figure~\ref{fig:rf-vimp}, we plot VIMP measures for each of the variables used to grow the forest estimates of Figure~\ref{fig:rfsrc-plot}. Variables are shown in VIMP rank order, largest (op\_yr) at the top, to smallest (iv\_lospr) at the bottom. In this case, we would focus attention on the top three variables (op\_yr (surgical date), ld and devno).
<<rf-vimp, echo = TRUE, fig.cap = "Variable Importance">>= 
plot.gg_vimp(pbc_rf) + 
 theme(legend.position = "none")
@

\subsection{Minimal Depth}\label{S:minimalDepth}
In VIMP, prognostic risk factors are determined by inspection of the forest, ranking the most important variables according to impact on predictive ability of the forest. An alternative method recognizes that most important variables for prediction are those that most frequently split nodes nearest to the trunks of the trees (ie, at the root node) since they partition the largest portions of the population. 

Node levels are numbered based on their relative distance to the trunk of the tree (ie. 0, 1, 2). A measure of important risk factors is determined by averaging the depth of first split for each variable over all trees within the forest. Lower values of this measure indicate variables that split larger groups of patients. 

The maximal subtree for a variable $x$ is the largest subtree whose root node splits on $x$. Thus, all parent nodes of $x$'s maximal subtree have nodes that split on variables other than $x$. The largest maximal subtree possible is the root node. In general, however, there can be more than one maximal subtree for a variable. A maximal subtree may also not exist if there are no splits on the variable. The minimal depth of a maximal subtree (the first order depth) measures predictiveness of a variable $x$. It equals the shortest distance (the depth) from the root node to the parent node of the maximal subtree (zero is the smallest value possible). The smaller the minimal depth, the more impact $x$ has on prediction. The mean of the minimal depth distribution is used as the threshold value for deciding whether a variable's minimal depth value is small enough for the variable to be classified as strong. 

The minimal depth plot of Figure~\ref{fig:mindepth} is similar to the VIMP plot in Figure~\ref{fig:rf-vimp}, ranking variables from most important at the top (minimal depth measure), to least at the bottom (maximal minimal depth). Since the VIMP and Minimal Depth measures use different criteria, we expect the variable ranking to be slightly different. In this case, minimal depth indicates seven most important variables (op\_yr (surgical date), age, ld, ht, wt, iv\_lospr (length of stay) and inr). The vertical dashed line indicates the minimal depth threshold where smaller minimal depth values indicate higher importance and larger indicate lower importance.


<<mindepth-view, eval = FALSE, echo = TRUE>>= 
pbc_vs <- var.select(pbc_rf)
ggMindepth <- gg_minimal_depth(pbc_vs)
print(ggMindepth)
@

<<mindepth-load>>= 
data(pbc_vs, package="ggRandomForests")
ggMindepth <- gg_minimal_depth(pbc_vs)
ggMindepth
@

<<mindepth-plot, echo=TRUE, fig.cap = "Minimal Depth Plot">>= 
plot(ggMindepth)
@


\section{Variable Dependence}\label{S:dependence}
Once we have an idea of which variables contribute to the predictive accuracy of the forest, it is useful to get some idea of form of this contribution. We use graphical methods to show the predicted response given dependence on covariates. We can plot the marginal effect of an covariate on the class probability (classification), response (regression), mortality (survival), or the expected years lost (competing risk) for a RF analysis. We plot the ensemble predicted value on the vertical axis and covariates along the horizontal axis.

\subsection{Marginal Dependence}\label{S:variableDependence}
\emph{Marginal variable dependence} plots the predicted response as a function of the covariate, showing each subject as a point on the plot. For classification and regression, this is straight forward predicting the response. In survival settings, we must account for the additional dimension of time. In this case, we plot the response at a specific time point of interest, for example survival at three months shown by the vertical dashed line in Figure~\ref{fig:rfsrc-plot3Mnth}. We take the predicted value of each curve at that time, and plot that against the covariate value for that observations, shown in Figure~\ref{fig:variable-plot}. Again censored cases are shown in blue circles, events are indicated by the red "x" symbols. Each predicted point is dependent on the full combination of all other covariates, not only on the covariate displayed in the dependence plot, so interpretation of these variable dependence plots can only be in general terms. The smooth loess line~\citep{cleveland:1981, cleveland:1988} indicates the trend of the prediction over surgical date progression.

<<rfsrc-plot3Mnth, echo = TRUE, fig.cap = "PBC Survival">>= 
ggRFsrc + 
 geom_vline(aes(xintercept = c(1, 3)), linetype = "dashed") + 
 coord_cartesian(x = c(0, 4))
@

<<variable-plotbili, echo = TRUE, fig.cap = "Variable dependence Survival vs. Bilirubin", fig.height = 4>>= 
xvar <- pbc_vs$topvars[1:6]
ind = 1
ggrf <- gg_variable(pbc_rf, time = c(1, 3), time.labels = c("1 Year", "3 Years"))

plot(ggrf, x_var = xvar[ind], se=FALSE, alpha=.3) + 
 labs(y = "Survival") + 
 theme(legend.position = "none") + 
 scale_color_manual(values = strCol, labels = event.labels) + 
 scale_shape_manual(values = event.marks, labels = event.labels)
@

<<variable-plotCombines, echo = TRUE, fig.cap = "Variable dependence Survival panel", fig.width = 7, fig.height = 4>>= 
plot(ggrf, x_var = xvar[c(2,3,5,6)], panel = TRUE, 
     se=FALSE, alpha=.3, 
     method="glm", formula=y~poly(x,2)) + 
 labs(y = "Survival") + 
 theme(legend.position = "none") + 
 scale_color_manual(values = strCol, labels = event.labels) + 
 scale_shape_manual(values = event.marks, labels = event.labels)+
  coord_cartesian(y=c(1,102))
@

\subsection{Partial Dependence}\label{S:partialDependence}

\emph{Partial dependence plots} are a risk adjusted alternative to marginal variable dependence. Partial plots are generated by integrating out the effects of variables beside the covariate of interest. The figures are constructed by selecting points evenly spaced along the distribution of the X variable. For each of these values (X = x), we calculate the average Random Forest prediction over all other covariates in X by \eqref{E:partial}.
\begin{equation}
\tilde{f}(x) = \frac{1}{n} \sum_{i = 1}^n \hat{f}(x, x_{i, o}), 
\label{E:partial}
\end{equation}
where $\hat{f}$ is the predicted response from the random forest and $x_{i, o}$ is the value for all other covariates other than $X = x$ for the observation $i$~\citep{FriedmanGreedyfunction:2000}. Partial dependence plots in time to event settings are shown at specific time points, similar to variable dependence.

Figure~\ref{fig:pbc-partial-bili} shows the partial dependence of three month freedom from thrombosis on the surgical date covariate. 

<<pbc-partial, echo = TRUE, eval = FALSE>>= 
# Calculate the 1 year partial dependence
pbc_prtl <- plot.variable(pbc_rf, surv.type = "surv", 
             time = 364.25, 
           xvar.names = xvar, partial = TRUE, 
           show.plots = FALSE)

# Calculate the 3 year partial dependence
pbc_prtl.3 <- plot.variable(pbc_rf, surv.type = "surv", 
              time = 3*364.25, 
           xvar.names = xvar, partial = TRUE, 
           show.plots = FALSE)

# Create gg_partial objects
ggPrtl <- gg_partial(pbc_prtl)
ggPrtl.3 <- gg_partial(pbc_prtl.3)

# Combine the objects to get multiple time curves 
# along variables on a single figure.
pbc_ggpart <- combine(ggPrtl, ggPrtl.3, 
           labels = c("1 Year", "3 Years"))

@

<<pbc-partial-load>>= 
data("pbc_prtl", package="ggRandomForests")
data("pbc_ggpart", package="ggRandomForests")
@


<<pbc-partial-bili, echo = TRUE, fig.cap = "Risk adjusted Survival">>= 
plot(pbc_ggpart[["bili"]], se = FALSE) + 
 theme(legend.position = c(.8, .5)) + 
 labs(y = "Survival", 
    x = dta.labs[which(rownames(dta.labs) ==  "bili"), "label"])
@

Non-proportional hazards are evident in Figure~\ref{fig:pbc-partial-bili}.


<<pbc-partial-panel, echo = TRUE, fig.cap = "Risk adjusted Survival - panel plot", fig.width = 7, fig.height = 4>>= 
pbc_ggpart$bili <- pbc_ggpart$edema <- NULL
plot(pbc_ggpart, se = FALSE, panel = TRUE) + 
  theme(legend.position=c(.6,.6))+
 labs(x = "", y = "Survival")
@


\section{Variable Interactions}\label{S:interactions}
Using the different variable dependence measures, we can calculate pairwise interactions for any pair of variables. Minimal depth is calculated as the maximal subtree using the normalized minimal depth of variable $i$ relative to the root node (normalized with respect to the size of the tree). For interactions, we calculate the maximal subtree interaction measure as the normalized minimal depth of a variable $j$ with respect to the maximal subtree for variable $i$ (normalized with respect to the size of $i$'s maximal subtree)~\citep{Ishwaran_HighDimension:2010,Ishwaran_HighDimension:2011}.

<<interaction-show, echo = TRUE, eval = FALSE>>= 
pbc_interaction <- find.interaction(pbc_rf)

ggint <- gg_interaction(pbc_interaction)
plot(ggint, x_var = "bili") + 
 labs(y = "Interactive Minimal Depth")
@

<<interaction, fig.cap = "Minimal Depth interaction for Surgical Date">>= 
 #pDat.int <- find.interaction(pbc_rf)
data("pbc_interaction", package="ggRandomForests")

plot(gg_interaction(pbc_interaction), x_var = "bili") + 
 labs(y = "Interactive Minimal Depth")
@

Measuring interactions with minimal depth results a $p \times p$ matrix of interaction measures, with smaller diagonal measures relative to the root node, and off diagonal measures of pairwise interaction. We expect the covariate with smallest minimal depth to have the highest interactive depth measures, so viewed alone may not be as informative as looking at other interactive depth plots. Figure~\ref{fig:interactionPanel} combines the remaining top ranked minimal depth measures for comparison.
<<interactionPanel, echo = TRUE, fig.cap = "Risk adjusted Survival - panel plot", fig.width = 7, fig.height = 4>>= 
plot(gg_interaction(pbc_interaction), x_var = xvar[2:5]) + 
 labs(y = "Interactive Minimal Depth") + 
 theme(legend.position = "none")
@

\subsection{Conditional Dependence Plots}

By plotting the resulting interaction measures for each variable (Figure~\ref{fig:interaction}), we can detect the "most interactive" pairs, and develop conditional plots~\cite{chambers:1992, cleveland:1993}. These plots are similar to stratified results, arranged in a set of panels by the interactive variable of interest. 

Interactions with categorical data are more straight forward, and can be generated directly from variable dependence plots. Recall the 1 year variable dependence for Billirubin, shown in Figure~\ref{fig:var_dep}. 
<<var_dep, echo = TRUE, fig.cap = "Bilirubin Variable Dependence at 1 year.">>= 
ggrf <- gg_variable(pbc_rf, time = 1)

ggvar <- ggrf
ggvar$treatment <- as.numeric(ggvar$treatment)
ggvar$treatment[which(ggvar$treatment==1)] <- "D-pen" 
ggvar$treatment[which(ggvar$treatment==2)] <- "placebo" 
ggvar$treatment <- factor(ggvar$treatment)

ggvar$stage <- paste("stage=", ggvar$stage, sep="")

var_dep <- plot(ggvar, x_var = "bili", smooth = TRUE, 
        method = "glm", alpha = .5, se = FALSE) + 
 labs(y = "Survival", x = dta.labs[which(rownames(dta.labs) ==  xvar[ind]), "label"]) + 
 theme(legend.position = "none") + 
 scale_color_manual(values = strCol, labels = event.labels) + 
 scale_shape_manual(values = event.marks, labels = event.labels)

show(var_dep)
@

We can view the conditional dependence of survival against bilirubin, versus other categorical covariates, say treatment (binary) and stage (categorical), by adding a facet argument.
<<coplot_bilirubin, echo = TRUE, fig.cap = "Conditional Variable Dependence. Interactions between bilrubin with treatment and stage variables.", fig.width = 7, fig.height = 4>>= 
var_dep + 
 facet_grid(treatment~stage)
@

Interactions with continuous variables requires stratification at some level.


\section{Conclusion}


%\singlespacing
\bibliography{ggRandomForests}

\end{document}